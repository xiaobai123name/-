# Transformer 架构详解

## 1. 简介

Transformer 是一种基于自注意力机制的神经网络架构，由 Google 在 2017 年的论文《Attention Is All You Need》中提出。它彻底改变了自然语言处理领域，成为了 GPT、BERT 等大型语言模型的基础。

### 1.1 为什么需要 Transformer？

传统的序列模型如 RNN 和 LSTM 存在以下问题：

- **顺序计算**：必须按顺序处理序列，无法并行化
- **长距离依赖**：难以捕获长序列中相距较远的依赖关系
- **梯度消失/爆炸**：在长序列上训练时容易出现梯度问题

Transformer 通过自注意力机制解决了这些问题。

## 2. 核心组件

### 2.1 自注意力机制 (Self-Attention)

自注意力机制允许模型在处理一个位置时，关注序列中的所有其他位置。

#### 计算过程

1. **生成 Q、K、V**：对于输入序列的每个位置，通过线性变换生成查询（Query）、键（Key）和值（Value）向量
2. **计算注意力分数**：Query 与所有 Key 做点积，得到注意力分数
3. **Softmax 归一化**：将分数转换为概率分布
4. **加权求和**：用注意力权重对 Value 进行加权求和

#### 公式

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$

其中 $d_k$ 是 Key 向量的维度，除以 $\sqrt{d_k}$ 是为了防止点积过大导致 softmax 梯度消失。

### 2.2 多头注意力 (Multi-Head Attention)

多头注意力将输入分成多个"头"，每个头独立进行注意力计算，然后将结果拼接。

**优势**：
- 允许模型在不同的表示子空间中关注不同的信息
- 增加模型的表达能力

### 2.3 位置编码 (Positional Encoding)

由于自注意力机制本身不包含位置信息，需要通过位置编码来引入序列的顺序信息。

原论文使用正弦和余弦函数：

$$
PE_{(pos, 2i)} = \sin(pos / 10000^{2i/d_{model}})
$$
$$
PE_{(pos, 2i+1)} = \cos(pos / 10000^{2i/d_{model}})
$$

## 3. 编码器-解码器结构

### 3.1 编码器 (Encoder)

编码器由 N 个相同的层堆叠而成，每层包含：

1. **多头自注意力层**：让编码器关注输入序列的不同位置
2. **前馈神经网络**：两个线性变换中间加 ReLU 激活
3. **残差连接与层归一化**：每个子层都有残差连接和层归一化

### 3.2 解码器 (Decoder)

解码器也由 N 个相同的层堆叠，每层包含：

1. **掩码多头自注意力**：防止关注未来位置
2. **编码器-解码器注意力**：关注编码器的输出（Cross-Attention）
3. **前馈神经网络**

#### 重要概念：Cross-Attention

在解码器中，Cross-Attention 层使用：
- **Query** 来自解码器的上一层输出
- **Key 和 Value** 来自编码器的输出

这使得解码器能够"参考"编码器理解的信息。

## 4. 训练技巧

### 4.1 学习率预热 (Warmup)

使用带预热的学习率调度：

$$
lrate = d_{model}^{-0.5} \cdot \min(step^{-0.5}, step \cdot warmup\_steps^{-1.5})
$$

### 4.2 标签平滑 (Label Smoothing)

使用标签平滑正则化来防止模型过度自信。

## 5. 应用

Transformer 架构广泛应用于：

- **机器翻译**：原始应用场景
- **文本生成**：GPT 系列模型
- **文本理解**：BERT 及其变体
- **图像处理**：Vision Transformer (ViT)
- **多模态**：CLIP、Flamingo 等

## 6. 总结

Transformer 的核心创新：

1. **自注意力机制**：允许捕获长距离依赖
2. **并行计算**：可以同时处理序列中的所有位置
3. **多头注意力**：增强模型的表达能力

这些创新使 Transformer 成为现代深度学习中最重要的架构之一。
